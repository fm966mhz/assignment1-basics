"""Tokenize an input text file using a BPE tokenizer and save the token IDs."""

import os

from multiprocessing import Pool
from typing import Any

import numpy as np
import regex as re

from absl import app
from absl import flags
from absl import logging
from tqdm import tqdm

from cs336_basics.bpe_tokenizer import BpeTokenizer
from cs336_basics.pretokenization_example import find_chunk_boundaries


FLAGS = flags.FLAGS
flags.DEFINE_string("input_file_path", None, "Path to the input text file to tokenize.")
flags.DEFINE_string(
    "output_token_ids_path", None, "Path to save the token IDs (numpy .npy file)."
)
flags.DEFINE_string(
    "vocab_path", None, "Path to the pickled vocabulary file for the tokenizer."
)
flags.DEFINE_string(
    "merges_path", None, "Path to the pickled merges file for the tokenizer."
)
flags.DEFINE_integer(
    "num_tokenizer_processes",
    8,
    "Number of processes to use for tokenization.",
)
flags.DEFINE_string(
    "split_special_token",
    "<|endoftext|>",
    "Special token used to split the input during pretokenization.",
)
flags.DEFINE_integer(
    "num_input_file_chunks",
    16,
    "Number of chunks to split the input file into for parallel tokenization.",
)
flags.DEFINE_bool(
    "clean_up_parts_files",
    True,
    "If True, clean up the parts files generated by individual tokenizer processes after they are"
    "mreged into the main output.",
)


def tokenize_one_chunk(kwargs: dict[str, Any]) -> str:
    """Tokenize a chunk of the input file from start to end byte offsets."""
    tokenizer = kwargs["tokenizer"]
    input_file_path = kwargs["input_file_path"]
    chunk_id = kwargs["chunk_id"]
    start = kwargs["start"]
    end = kwargs["end"]
    output_file_path_prefix = kwargs["output_file_path_prefix"]
    tmp_input_file_path = f"{input_file_path}-part-{chunk_id}"
    output_file_path = f"{output_file_path_prefix}-part-{chunk_id}"
    with (
        open(input_file_path, "rb") as input_file,
        open(tmp_input_file_path, "wb") as tmp_input_file,
    ):
        input_file.seek(start)
        chunk_bytes = input_file.read(end - start)
        tmp_input_file.write(chunk_bytes)
        del chunk_bytes
    output_token_ids = []
    with open(tmp_input_file_path, "r", encoding="utf-8") as input_file:
        for token_id in tqdm(tokenizer.encode_iterable(input_file)):
            output_token_ids.append(token_id)
    os.remove(tmp_input_file_path)
    with open(output_file_path, "wb") as output_file:
        np.save(output_file, np.array(output_token_ids, dtype=np.uint16))
    return output_file_path


def merge_npy_part_files(
    all_file_paths: list[str], clean_up_parts_files: bool = True
) -> np.ndarray:
    """Merge npy part files."""
    sorted_file_paths = []
    for file_path in all_file_paths:
        file_path_parts = re.split("-part-", file_path)
        chunk_id = int(file_path_parts[-1])
        sorted_file_paths.append((chunk_id, file_path))
    sorted_file_paths = sorted(sorted_file_paths)
    sorted_token_ids = []
    for _, file_path in tqdm(sorted_file_paths, desc="Merge numpy output parts"):
        with open(file_path, "rb") as f:
            sorted_token_ids.append(np.load(f))
    if clean_up_parts_files:
        for _, file_path in sorted_file_paths:
            os.remove(file_path)
    return np.concatenate(sorted_token_ids)


def main(argv):
    """Main function to tokenize the input file and save token IDs."""
    if len(argv) > 1:
        raise app.UsageError("Too many command-line arguments.")

    if FLAGS.input_file_path is None:
        raise app.UsageError("Must specify --input_file_path")

    if FLAGS.output_token_ids_path is None:
        raise app.UsageError("Must specify --output_token_ids_path")

    if FLAGS.vocab_path is None:
        raise app.UsageError("Must specify --vocab_path")

    if FLAGS.merges_path is None:
        raise app.UsageError("Must specify --merges_path")

    with open(FLAGS.input_file_path, "rb") as f:
        all_chunk_boundaries = find_chunk_boundaries(
            f,
            desired_num_chunks=FLAGS.num_input_file_chunks,
            split_special_token=FLAGS.split_special_token.encode("utf-8"),
        )

    logging.info("Loading BPE tokenizer...")
    tokenizer = BpeTokenizer.from_files(
        vocab_filepath=FLAGS.vocab_path,
        merges_filepath=FLAGS.merges_path,
    )

    logging.info("Tokenizing input file in parallel...")
    with Pool(processes=FLAGS.num_tokenizer_processes) as pool:
        parts_output_file_paths = list(
            tqdm(
                pool.imap_unordered(
                    tokenize_one_chunk,
                    iterable=[
                        {
                            "tokenizer": tokenizer,
                            "input_file_path": FLAGS.input_file_path,
                            "chunk_id": i,
                            "start": all_chunk_boundaries[i],
                            "end": all_chunk_boundaries[i + 1],
                            "output_file_path_prefix": FLAGS.output_token_ids_path,
                        }
                        for i in range(len(all_chunk_boundaries) - 1)
                    ],
                ),
                total=len(all_chunk_boundaries) - 1,
            )
        )

    logging.info("Assembling token part files...")
    output_tokens = merge_npy_part_files(parts_output_file_paths)
    logging.info(f"Writing tokens to file {FLAGS.output_token_ids_path}")
    np.save(FLAGS.output_token_ids_path, output_tokens.astype(np.uint16))
    logging.info("Tokenization complete.")


if __name__ == "__main__":
    app.run(main)
